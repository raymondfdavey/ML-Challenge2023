{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import os\n",
    "# os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import utils_ray\n",
    "\n",
    "from utils_ray import scale_columns, create_windows, append_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils_ray) \n",
    "from utils_ray import scale_columns, create_windows, append_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movementSensorData.csv\")\n",
    "df = df.rename(columns={'Unnamed: 0': 'time_ms'})\n",
    "df = append_segments(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "\n",
    "# Replace specific activity value\n",
    "df_copy['activity'].replace({77: 0}, inplace=True)\n",
    "\n",
    "# Normalize specific columns\n",
    "columns_to_normalize = ['lw_x', 'lw_y', 'lw_z']\n",
    "scaler, df_copy = scale_columns(df_copy, columns_to_normalize)\n",
    "\n",
    "# Creating windows with optimized function\n",
    "window_size = 5\n",
    "X, y = create_windows(df_copy, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42) \n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(355464, 5, 3)\n",
      "(76171, 5, 3)\n",
      "(76172, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lw_x</th>\n",
       "      <th>lw_y</th>\n",
       "      <th>lw_z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.879</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.852</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.879</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.039</td>\n",
       "      <td>-1.012</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lw_x   lw_y   lw_z  activity\n",
       "0 -0.188 -0.941 -0.316         2\n",
       "1 -0.121 -0.879 -0.320         2\n",
       "2 -0.070 -0.852 -0.305         2\n",
       "3 -0.023 -0.879 -0.277         2\n",
       "4  0.008 -0.941 -0.242         2\n",
       "5  0.039 -1.012 -0.191         2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INSPECT DATA ORIGINAL VERSUS WINDOWS\n",
    "df[['lw_x',\t'lw_y'\t,'lw_z', 'activity']].head(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04446353 -1.26252832  0.36440705]\n",
      " [ 0.17230926 -1.1773578   0.35538717]\n",
      " [ 0.26962468 -1.14026741  0.38921172]\n",
      " [ 0.35930751 -1.1773578   0.4523509 ]\n",
      " [ 0.41846001 -1.26252832  0.53127487]]\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.188, -0.941, -0.316],\n",
       "       [-0.121, -0.879, -0.32 ],\n",
       "       [-0.07 , -0.852, -0.305],\n",
       "       [-0.023, -0.879, -0.277],\n",
       "       [ 0.008, -0.941, -0.242]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(y[0])\n",
    "scaler.inverse_transform(X[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Determine the number of unique classes\n",
    "num_classes = df['activity'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11109/11109 [==============================] - 33s 3ms/step - loss: 0.2592 - accuracy: 0.9208 - val_loss: 0.2294 - val_accuracy: 0.9275\n",
      "Epoch 2/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.2288 - accuracy: 0.9274 - val_loss: 0.2374 - val_accuracy: 0.9199\n",
      "Epoch 3/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.2213 - accuracy: 0.9290 - val_loss: 0.2185 - val_accuracy: 0.9299\n",
      "Epoch 4/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.2150 - accuracy: 0.9304 - val_loss: 0.2083 - val_accuracy: 0.9328\n",
      "Epoch 5/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.2100 - accuracy: 0.9315 - val_loss: 0.2113 - val_accuracy: 0.9318\n",
      "Epoch 6/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.2048 - accuracy: 0.9331 - val_loss: 0.2043 - val_accuracy: 0.9333\n",
      "Epoch 7/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.2002 - accuracy: 0.9343 - val_loss: 0.1991 - val_accuracy: 0.9347\n",
      "Epoch 8/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.1958 - accuracy: 0.9358 - val_loss: 0.1944 - val_accuracy: 0.9360\n",
      "Epoch 9/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.1911 - accuracy: 0.9368 - val_loss: 0.1931 - val_accuracy: 0.9366\n",
      "Epoch 10/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.1869 - accuracy: 0.9382 - val_loss: 0.1875 - val_accuracy: 0.9377\n",
      "Epoch 11/100\n",
      "11109/11109 [==============================] - 31s 3ms/step - loss: 0.1823 - accuracy: 0.9394 - val_loss: 0.1833 - val_accuracy: 0.9385\n",
      "Epoch 12/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.1786 - accuracy: 0.9405 - val_loss: 0.1801 - val_accuracy: 0.9404\n",
      "Epoch 13/100\n",
      "11109/11109 [==============================] - 31s 3ms/step - loss: 0.1747 - accuracy: 0.9412 - val_loss: 0.1819 - val_accuracy: 0.9393\n",
      "Epoch 14/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.1711 - accuracy: 0.9423 - val_loss: 0.1766 - val_accuracy: 0.9408\n",
      "Epoch 15/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.1678 - accuracy: 0.9434 - val_loss: 0.1719 - val_accuracy: 0.9414\n",
      "Epoch 16/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.1642 - accuracy: 0.9443 - val_loss: 0.1702 - val_accuracy: 0.9423\n",
      "Epoch 17/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.1609 - accuracy: 0.9453 - val_loss: 0.1700 - val_accuracy: 0.9427\n",
      "Epoch 18/100\n",
      "11109/11109 [==============================] - 31s 3ms/step - loss: 0.1582 - accuracy: 0.9462 - val_loss: 0.1690 - val_accuracy: 0.9427\n",
      "Epoch 19/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.1554 - accuracy: 0.9469 - val_loss: 0.1673 - val_accuracy: 0.9437\n",
      "Epoch 20/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.1527 - accuracy: 0.9478 - val_loss: 0.1645 - val_accuracy: 0.9443\n",
      "Epoch 21/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.1499 - accuracy: 0.9486 - val_loss: 0.1635 - val_accuracy: 0.9451\n",
      "Epoch 22/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.1473 - accuracy: 0.9497 - val_loss: 0.1619 - val_accuracy: 0.9444\n",
      "Epoch 23/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.1449 - accuracy: 0.9506 - val_loss: 0.1608 - val_accuracy: 0.9450\n",
      "Epoch 24/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.1422 - accuracy: 0.9509 - val_loss: 0.1618 - val_accuracy: 0.9463\n",
      "Epoch 25/100\n",
      "11109/11109 [==============================] - 31s 3ms/step - loss: 0.1402 - accuracy: 0.9517 - val_loss: 0.1594 - val_accuracy: 0.9460\n",
      "Epoch 26/100\n",
      "11109/11109 [==============================] - 31s 3ms/step - loss: 0.1380 - accuracy: 0.9522 - val_loss: 0.1571 - val_accuracy: 0.9466\n",
      "Epoch 27/100\n",
      "11109/11109 [==============================] - 30s 3ms/step - loss: 0.1361 - accuracy: 0.9532 - val_loss: 0.1571 - val_accuracy: 0.9464\n",
      "Epoch 28/100\n",
      "11109/11109 [==============================] - 32s 3ms/step - loss: 0.1340 - accuracy: 0.9539 - val_loss: 0.1581 - val_accuracy: 0.9463\n",
      "Epoch 29/100\n",
      "11109/11109 [==============================] - 31s 3ms/step - loss: 0.1325 - accuracy: 0.9541 - val_loss: 0.1579 - val_accuracy: 0.9469\n",
      "Epoch 30/100\n",
      "11109/11109 [==============================] - 31s 3ms/step - loss: 0.1305 - accuracy: 0.9548 - val_loss: 0.1562 - val_accuracy: 0.9472\n",
      "Epoch 31/100\n",
      "11109/11109 [==============================] - 31s 3ms/step - loss: 0.1287 - accuracy: 0.9549 - val_loss: 0.1535 - val_accuracy: 0.9477\n",
      "Epoch 32/100\n",
      "11109/11109 [==============================] - 31s 3ms/step - loss: 0.1272 - accuracy: 0.9556 - val_loss: 0.1535 - val_accuracy: 0.9488\n",
      "Epoch 33/100\n",
      "11109/11109 [==============================] - 31s 3ms/step - loss: 0.1253 - accuracy: 0.9561 - val_loss: 0.1538 - val_accuracy: 0.9483\n",
      "Epoch 34/100\n",
      "11109/11109 [==============================] - 31s 3ms/step - loss: 0.1238 - accuracy: 0.9567 - val_loss: 0.1541 - val_accuracy: 0.9482\n",
      "Epoch 35/100\n",
      "11109/11109 [==============================] - 32s 3ms/step - loss: 0.1224 - accuracy: 0.9569 - val_loss: 0.1537 - val_accuracy: 0.9483\n",
      "Epoch 36/100\n",
      "11109/11109 [==============================] - 33s 3ms/step - loss: 0.1209 - accuracy: 0.9575 - val_loss: 0.1545 - val_accuracy: 0.9487\n",
      "Epoch 37/100\n",
      "11108/11109 [============================>.] - ETA: 0s - loss: 0.1196 - accuracy: 0.9582Restoring model weights from the end of the best epoch: 32.\n",
      "11109/11109 [==============================] - 31s 3ms/step - loss: 0.1196 - accuracy: 0.9582 - val_loss: 0.1550 - val_accuracy: 0.9482\n",
      "Epoch 37: early stopping\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.1553 - accuracy: 0.9477\n",
      "Test Loss: 0.1553150713443756, Test Accuracy: 0.9476841688156128\n"
     ]
    }
   ],
   "source": [
    "# Building the LSTM model for multi-class classification\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(window_size, 3)))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Configure Early Stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=3, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Training the model with validation data and Early Stopping\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(\n",
    "    X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Ensure X_test and y_test are defined and preprocessed as they were for training\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# If your model outputs probabilities (common in classification), you might need to convert these to class labels\n",
    "# Adjust if your model's output differs\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Additionally, print a classification report for more insights\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predicted_labels))\n",
    "print(classification_report(y_test, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11108/11108 [==============================] - 47s 4ms/step - loss: 0.2453 - accuracy: 0.9242 - val_loss: 0.2164 - val_accuracy: 0.9312\n",
      "Epoch 2/100\n",
      "11108/11108 [==============================] - 45s 4ms/step - loss: 0.2069 - accuracy: 0.9335 - val_loss: 0.1958 - val_accuracy: 0.9366\n",
      "Epoch 3/100\n",
      "11108/11108 [==============================] - 44s 4ms/step - loss: 0.1867 - accuracy: 0.9392 - val_loss: 0.1805 - val_accuracy: 0.9414\n",
      "Epoch 4/100\n",
      "11108/11108 [==============================] - 44s 4ms/step - loss: 0.1728 - accuracy: 0.9431 - val_loss: 0.1658 - val_accuracy: 0.9451\n",
      "Epoch 5/100\n",
      "11108/11108 [==============================] - 47s 4ms/step - loss: 0.1617 - accuracy: 0.9461 - val_loss: 0.1588 - val_accuracy: 0.9473\n",
      "Epoch 6/100\n",
      "11108/11108 [==============================] - 48s 4ms/step - loss: 0.1521 - accuracy: 0.9492 - val_loss: 0.1506 - val_accuracy: 0.9497\n",
      "Epoch 7/100\n",
      "11108/11108 [==============================] - 44s 4ms/step - loss: 0.1443 - accuracy: 0.9515 - val_loss: 0.1459 - val_accuracy: 0.9510\n",
      "Epoch 8/100\n",
      "11108/11108 [==============================] - 44s 4ms/step - loss: 0.1364 - accuracy: 0.9537 - val_loss: 0.1386 - val_accuracy: 0.9538\n",
      "Epoch 9/100\n",
      "11108/11108 [==============================] - 45s 4ms/step - loss: 0.1298 - accuracy: 0.9559 - val_loss: 0.1363 - val_accuracy: 0.9544\n",
      "Epoch 10/100\n",
      "11108/11108 [==============================] - 47s 4ms/step - loss: 0.1240 - accuracy: 0.9576 - val_loss: 0.1326 - val_accuracy: 0.9549\n",
      "Epoch 11/100\n",
      "11108/11108 [==============================] - 47s 4ms/step - loss: 0.1188 - accuracy: 0.9592 - val_loss: 0.1237 - val_accuracy: 0.9584\n",
      "Epoch 12/100\n",
      " 7676/11108 [===================>..........] - ETA: 13s - loss: 0.1132 - accuracy: 0.9609"
     ]
    }
   ],
   "source": [
    "for window_size in [10, 20]:    \n",
    "    X, y = create_windows(df_copy, window_size)# Splitting the dataset into training, validation, and testing sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Determine the number of unique classes\n",
    "    num_classes = df['activity'].nunique()\n",
    "        # Building the LSTM model for multi-class classification\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(window_size, 3)))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Configure Early Stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', patience=3, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "    # Training the model with validation data and Early Stopping\n",
    "    model.fit(X_train, y_train, epochs=100, validation_data=(\n",
    "        X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print('WINDOW SIZE = ', window_size)\n",
    "    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "    # Ensure X_test and y_test are defined and preprocessed as they were for training\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # If your model outputs probabilities (common in classification), you might need to convert these to class labels\n",
    "    # Adjust if your model's output differs\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Additionally, print a classification report for more insights\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, predicted_labels))\n",
    "    print(classification_report(y_test, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer 'lstm_cell' expected 3 variables, but received 0 variables during loading. Expected: ['lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiles/model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m---> 14\u001b[0m     model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Ensure X_test and y_test are defined and preprocessed as they were for training\u001b[39;00m\n\u001b[0;32m     17\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\pickle_utils.py:48\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[1;34m(serialized_model)\u001b[0m\n\u001b[0;32m     46\u001b[0m     model \u001b[38;5;241m=\u001b[39m saving_lib\u001b[38;5;241m.\u001b[39mload_model(filepath, safe_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\pickle_utils.py:46\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[1;34m(serialized_model)\u001b[0m\n\u001b[0;32m     40\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(serialized_model)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# When loading, direct import will work for most custom objects\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# though it will require get_config() to be implemented.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Some custom objects (e.g. an activation in a Dense layer,\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# serialized as a string by Dense.get_config()) will require\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# a custom_object_scope.\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     model \u001b[38;5;241m=\u001b[39m saving_lib\u001b[38;5;241m.\u001b[39mload_model(filepath, safe_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:275\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    272\u001b[0m             asset_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:263\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     asset_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m _load_state(\n\u001b[0;32m    264\u001b[0m     model,\n\u001b[0;32m    265\u001b[0m     weights_store\u001b[38;5;241m=\u001b[39mweights_store,\n\u001b[0;32m    266\u001b[0m     assets_store\u001b[38;5;241m=\u001b[39masset_store,\n\u001b[0;32m    267\u001b[0m     inner_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    268\u001b[0m     visited_trackables\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(),\n\u001b[0;32m    269\u001b[0m )\n\u001b[0;32m    270\u001b[0m weights_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m asset_store:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:456\u001b[0m, in \u001b[0;36m_load_state\u001b[1;34m(trackable, weights_store, assets_store, inner_path, skip_mismatch, visited_trackables)\u001b[0m\n\u001b[0;32m    447\u001b[0m     _load_state(\n\u001b[0;32m    448\u001b[0m         child_obj,\n\u001b[0;32m    449\u001b[0m         weights_store,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    453\u001b[0m         visited_trackables\u001b[38;5;241m=\u001b[39mvisited_trackables,\n\u001b[0;32m    454\u001b[0m     )\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child_obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mset\u001b[39m)):\n\u001b[1;32m--> 456\u001b[0m     _load_container_state(\n\u001b[0;32m    457\u001b[0m         child_obj,\n\u001b[0;32m    458\u001b[0m         weights_store,\n\u001b[0;32m    459\u001b[0m         assets_store,\n\u001b[0;32m    460\u001b[0m         inner_path\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(inner_path, child_attr),\n\u001b[0;32m    461\u001b[0m         skip_mismatch\u001b[38;5;241m=\u001b[39mskip_mismatch,\n\u001b[0;32m    462\u001b[0m         visited_trackables\u001b[38;5;241m=\u001b[39mvisited_trackables,\n\u001b[0;32m    463\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:513\u001b[0m, in \u001b[0;36m_load_container_state\u001b[1;34m(container, weights_store, assets_store, inner_path, skip_mismatch, visited_trackables)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    512\u001b[0m     used_names[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 513\u001b[0m _load_state(\n\u001b[0;32m    514\u001b[0m     trackable,\n\u001b[0;32m    515\u001b[0m     weights_store,\n\u001b[0;32m    516\u001b[0m     assets_store,\n\u001b[0;32m    517\u001b[0m     inner_path\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(inner_path, name),\n\u001b[0;32m    518\u001b[0m     skip_mismatch\u001b[38;5;241m=\u001b[39mskip_mismatch,\n\u001b[0;32m    519\u001b[0m     visited_trackables\u001b[38;5;241m=\u001b[39mvisited_trackables,\n\u001b[0;32m    520\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:447\u001b[0m, in \u001b[0;36m_load_state\u001b[1;34m(trackable, weights_store, assets_store, inner_path, skip_mismatch, visited_trackables)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child_attr, child_obj \u001b[38;5;129;01min\u001b[39;00m _walk_trackable(trackable):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_keras_trackable(child_obj):\n\u001b[1;32m--> 447\u001b[0m         _load_state(\n\u001b[0;32m    448\u001b[0m             child_obj,\n\u001b[0;32m    449\u001b[0m             weights_store,\n\u001b[0;32m    450\u001b[0m             assets_store,\n\u001b[0;32m    451\u001b[0m             inner_path\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(inner_path, child_attr),\n\u001b[0;32m    452\u001b[0m             skip_mismatch\u001b[38;5;241m=\u001b[39mskip_mismatch,\n\u001b[0;32m    453\u001b[0m             visited_trackables\u001b[38;5;241m=\u001b[39mvisited_trackables,\n\u001b[0;32m    454\u001b[0m         )\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child_obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mset\u001b[39m)):\n\u001b[0;32m    456\u001b[0m         _load_container_state(\n\u001b[0;32m    457\u001b[0m             child_obj,\n\u001b[0;32m    458\u001b[0m             weights_store,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    462\u001b[0m             visited_trackables\u001b[38;5;241m=\u001b[39mvisited_trackables,\n\u001b[0;32m    463\u001b[0m         )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:425\u001b[0m, in \u001b[0;36m_load_state\u001b[1;34m(trackable, weights_store, assets_store, inner_path, skip_mismatch, visited_trackables)\u001b[0m\n\u001b[0;32m    418\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    419\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load weights in object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrackable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    420\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    421\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    422\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    423\u001b[0m             )\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 425\u001b[0m         trackable\u001b[38;5;241m.\u001b[39mload_own_variables(weights_store\u001b[38;5;241m.\u001b[39mget(inner_path))\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trackable, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_assets\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m assets_store:\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_mismatch:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:3539\u001b[0m, in \u001b[0;36mLayer.load_own_variables\u001b[1;34m(self, store)\u001b[0m\n\u001b[0;32m   3537\u001b[0m all_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_weights \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_trainable_weights\n\u001b[0;32m   3538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_vars):\n\u001b[1;32m-> 3539\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3540\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables during loading. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3543\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[v\u001b[38;5;241m.\u001b[39mname\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mv\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mall_vars]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3544\u001b[0m     )\n\u001b[0;32m   3545\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_vars):\n\u001b[0;32m   3546\u001b[0m     \u001b[38;5;66;03m# TODO(rchao): check shapes and raise errors.\u001b[39;00m\n\u001b[0;32m   3547\u001b[0m     v\u001b[38;5;241m.\u001b[39massign(store[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: Layer 'lstm_cell' expected 3 variables, but received 0 variables during loading. Expected: ['lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0']"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Determine the number of unique classes\n",
    "num_classes = df['activity'].nunique()\n",
    "\n",
    "# Load the model from the pickle file\n",
    "filename = 'files/model.pkl'\n",
    "with open(filename, 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# Ensure X_test and y_test are defined and preprocessed as they were for training\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# If your model outputs probabilities (common in classification), you might need to convert these to class labels\n",
    "# Adjust if your model's output differs\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Additionally, print a classification report for more insights\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predicted_labels))\n",
    "print(classification_report(y_test, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 4\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(model, file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "filename = 'model.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "    \n",
    "import pickle\n",
    "filename = 'files/scaler.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "# Check if a GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "from utils_ray import scale_columns, create_windows, append_segments\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=32):\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movementSensorData.csv\")\n",
    "df = df.rename(columns={'Unnamed: 0': 'time_ms'})\n",
    "df = append_segments(df)\n",
    "df_copy = df.copy()\n",
    "df_copy['activity'].replace({77: 0}, inplace=True)\n",
    "# Normalize specific columns\n",
    "columns_to_normalize = ['lw_x', 'lw_y', 'lw_z']\n",
    "scaler, df_copy = scale_columns(df_copy, columns_to_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5  # Update this with your actual number of classes\n",
    "input_size = 3  # Update this with your actual number of features\n",
    "hidden_size = 50\n",
    "num_epochs = 100\n",
    "patience = 3\n",
    "batch_size = 32\n",
    "window_size = 100\n",
    "\n",
    "X, y = create_windows(df_copy, window_size)\n",
    "\n",
    "# Splitting the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "train_loader, val_loader, test_loader = create_dataloaders(X_train, y_train, X_val, y_val, X_test, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(355132, 100, 3)\n",
      "(76100, 100, 3)\n",
      "(76100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SUDefEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
